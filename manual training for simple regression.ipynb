{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"manual training for simple regression.ipynb","provenance":[{"file_id":"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/customization/custom_training.ipynb","timestamp":1589164131947}],"private_outputs":true,"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"id":"T4Ukk59gkpZh","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","import numpy as np\n","\n","\n","class Model(object):\n","  def __init__(self):\n","    # Initialize the weights to `5.0` and the bias to `0.0`\n","    # In practice, these should be initialized to random values (for example, with `tf.random.normal`)\n","    self.W = tf.Variable(5.0)\n","    self.b = tf.Variable(0.0)\n","\n","  def __call__(self, x):\n","    return self.W * x + self.b\n","\n","model = Model()\n","\n","\n","def loss(target_y, predicted_y):\n","  return tf.reduce_mean(tf.square(target_y - predicted_y))\n","\n","TRUE_W = 3.0\n","TRUE_b = 2.0\n","NUM_EXAMPLES = 1000\n","\n","inputs  = tf.random.normal(shape=[NUM_EXAMPLES])\n","noise   = tf.random.normal(shape=[NUM_EXAMPLES])\n","outputs = inputs * TRUE_W + TRUE_b + noise\n","\n","import matplotlib.pyplot as plt\n","\n","plt.scatter(inputs, outputs, c='b')\n","plt.scatter(inputs, model(inputs), c='r')\n","plt.show()\n","\n","print('Current loss: %1.6f' % loss(model(inputs), outputs).numpy())\n","\n","def train(model, inputs, outputs, learning_rate):\n","  with tf.GradientTape() as t:\n","    current_loss = loss(outputs, model(inputs))\n","  dW, db = t.gradient(current_loss, [model.W, model.b])\n","  model.W.assign_sub(learning_rate * dW)\n","  model.b.assign_sub(learning_rate * db)\n","\n","model = Model()\n","\n","# Collect the history of W-values and b-values to plot later\n","Ws, bs = [], []\n","epochs = range(10)\n","for epoch in epochs:\n","  Ws.append(model.W.numpy())\n","  bs.append(model.b.numpy())\n","  current_loss = loss(outputs, model(inputs))\n","\n","  train(model, inputs, outputs, learning_rate=0.1)\n","  print('Epoch %2d: W=%1.2f b=%1.2f, loss=%2.5f' %\n","        (epoch, Ws[-1], bs[-1], current_loss))\n","\n","# Let's plot it all\n","plt.plot(epochs, Ws, 'r',\n","         epochs, bs, 'b')\n","plt.plot([TRUE_W] * len(epochs), 'r--',\n","         [TRUE_b] * len(epochs), 'b--')\n","plt.legend(['W', 'b', 'True W', 'True b'])\n","plt.show()\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7V0-bHDokpYx","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","from tensorflow import keras\n","\n","import numpy as np\n","\n","data_size = 1000\n","# 80% of the data is for training.\n","train_pct = 0.8\n","\n","train_size = int(data_size * train_pct)\n","\n","# Create some input data between -1 and 1 and randomize it.\n","x = np.linspace(-1, 1, data_size)\n","np.random.shuffle(x)\n","\n","# Generate the output data.\n","# y = 0.5x + 2 + noise\n","y = 0.5 * x + 2 + np.random.normal(0, 0.05, (data_size, ))\n","\n","# Split into test and train pairs.\n","x_train, y_train = x[:train_size], y[:train_size]\n","x_test, y_test = x[train_size:], y[train_size:]\n","\n","\n","model = keras.models.Sequential([\n","    keras.layers.Dense(16, input_dim=1),\n","    keras.layers.Dense(1),\n","])\n","\n","model.compile(\n","    loss='mse', # keras.losses.mean_squared_error\n","    optimizer=keras.optimizers.SGD(lr=0.2),\n",")\n","\n","print(\"Training ... With default parameters, this takes less than 10 seconds.\")\n","training_history = model.fit(\n","    x_train, # input\n","    y_train, # output\n","    batch_size=train_size,\n","    verbose=0, # Suppress chatty output; use Tensorboard instead\n","    epochs=100,\n","    validation_data=(x_test, y_test),\n",")\n","\n","print(\"Average test loss: \", np.average(training_history.history['loss']))"],"execution_count":0,"outputs":[]}]}